{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/raoulg/straattaal/blob/main/notebooks/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "   import google.colab\n",
    "   IN_COLAB = True\n",
    "except ImportError:\n",
    "   IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "   !pip install slanggen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from slanggen import datatools\n",
    "from slanggen import models\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I scraped the \"Amsterdamse straattaal woordenboek\" , see [link](https://www.mijnwoordenboek.nl/dialect/Amsterdamse%20straattaal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_words = datatools.load_data(Path(\"../assets/straattaal.txt\"))\n",
    "processed_words[:10], len(processed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 453 words in total. I added a start `<s>` and stop `</s>` tag.\n",
    "We will use a Byte Pair Encoding (BPE) algorithm to learn the subword units from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = models.buildBPE(corpus=processed_words, vocab_size=100)\n",
    "list(tokenizer.get_vocab())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now encode a word and see which tokens are created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer.encode(\"waggie\")\n",
    "enc.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reconstruct the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(enc.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the sequences. We will:\n",
    "- transform words into subtokens, and then into arbitrary integers\n",
    "- add zeros to make all sequences the same length (padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = datatools.preprocess(processed_words, tokenizer)\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every word now is a list of integers. We will shift the sequence one position, such that the target (to predict) is the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datatools.ShiftedDataset(padded_sequences)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset[0]\n",
    "print(f\"input: {x}\")\n",
    "print(f\"output: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use a Dataloader. This will batch the sequences and shuffle the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "x, y = next(iter(loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will use the vocabulary size to use as an output size for the model.\n",
    "The model now takes:\n",
    "- as input: a sequence of integers\n",
    "- as output: for every possible BPE token, the probability that it is the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocab size based on the tokenizer\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set up all the ingredients:\n",
    "- the model uses 64 dimensions to represent the language\n",
    "- we can calculate the loss (the difference between the actual next token and the predicted next token)\n",
    "- the optimizer will tell the model in which direction to adjust the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "from torch import nn, optim\n",
    "# Hyperparameters\n",
    "config = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"output_dim\": vocab_size,\n",
    "}\n",
    "\n",
    "model = models.SlangRNN(config)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=50, min_lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train for 800 epochs. This means we will present the full dataset of 453 words for 800 times to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import torch\n",
    "\n",
    "epochs = 800\n",
    "history = []\n",
    "last_lr = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(x)\n",
    "        # input_seq, target_seq = dataset[i]\n",
    "\n",
    "        output, hidden = model(x, hidden)\n",
    "\n",
    "        loss += loss_fn(output.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    history.append(loss.item())\n",
    "    curr_lr = scheduler.get_last_lr()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        logger.info(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "        if last_lr != curr_lr:\n",
    "            last_lr = curr_lr\n",
    "            logger.info(f\"Current learning rate: {curr_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylim(0, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save, and load, the trained model for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../artefacts/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SlangRNN(config)\n",
    "model.load_state_dict(torch.load(\"../artefacts/model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now give a starting letter, eg 'a', and give the model a sequence of start_token and start_letter.\n",
    "The model will now start to predict next tokens, until it predicts the stop_token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_letter = 'a'\n",
    "max_length = 20\n",
    "temperature = 1.0\n",
    "start_token_idx = tokenizer.encode(\"<s>\").ids[0]\n",
    "start_letter_idx = tokenizer.encode(start_letter).ids[0]\n",
    "input_seq = torch.tensor([[start_token_idx, start_letter_idx]], dtype=torch.long)\n",
    "\n",
    "generated_word = [start_letter_idx]\n",
    "hidden = model.init_hidden(input_seq)\n",
    "for _ in range(max_length - 1):\n",
    "    with torch.no_grad():\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "    output = output.squeeze(0)\n",
    "    output = output[-1, :].view(-1).div(temperature).exp()\n",
    "    next_token = torch.multinomial(output, 1).item()\n",
    "    if next_token == tokenizer.token_to_id(\"<pad>\"):\n",
    "        break\n",
    "    generated_word.append(next_token)\n",
    "    input_seq = torch.tensor([generated_word], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can decode into a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop this process to generate multiple words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.sample_n(processed_words, n=10, model=model, tokenizer=tokenizer, max_length=20, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save everything for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"../artefacts/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../artefacts/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
