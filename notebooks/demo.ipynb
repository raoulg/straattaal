{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<td>\n",
    "<a href=\"https://colab.research.google.com/github/raoulg/straattaal/blob/main/notebooks/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary dependencies\n",
    "try:\n",
    "   import google.colab\n",
    "   IN_COLAB = True\n",
    "except ImportError:\n",
    "   IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "   !pip install slanggen\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from slanggen import datatools\n",
    "from slanggen import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I scraped the \"Amsterdamse straattaal woordenboek\" , see [link](https://www.mijnwoordenboek.nl/dialect/Amsterdamse%20straattaal).\n",
    "I will download the result from the scraping to train the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, datafile: Path):\n",
    "    datadir = datafile.parent\n",
    "    if not datadir.exists():\n",
    "        print(f\"Creating directory {datadir}\")\n",
    "        datadir.mkdir(parents=True)\n",
    "\n",
    "    if not datafile.exists():\n",
    "        print(f\"Downloading {url} to {datafile}\")\n",
    "        response = requests.get(url)\n",
    "        with datafile.open(\"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"File {datafile} already exists, skipping download\")\n",
    "\n",
    "url = \"https://github.com/raoulg/straattaal/blob/main/assets/straattaal.txt?raw=true\"\n",
    "datafile = Path(\"data/straattaal.txt\")\n",
    "download(url, datafile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the first ten words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_words = datatools.load_data(datafile)\n",
    "processed_words[:10], len(processed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 453 words in total. You can notice there is an extra start `<s>` and stop `</s>` tag, which will be used to train the model.\n",
    "We will use a Byte Pair Encoding (BPE) algorithm to learn the subword units from the corpus.\n",
    "\n",
    "Let's have a look at the first ten tokens, generated by the BPE algorithm from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = models.buildBPE(corpus=processed_words, vocab_size=100)\n",
    "list(tokenizer.get_vocab())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see a token is somewhere in between a word and a character. \n",
    "We can now encode a word and see which tokens are created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer.encode(\"waggie\")\n",
    "print(f'The subtokens of \"waggie\" are\\n {enc.tokens} \\nwith ids\\n {enc.ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reconstruct the word from the ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(enc.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the sequences. We will:\n",
    "- transform words into subtokens, and then into arbitrary integers\n",
    "- add zeros to make all sequences the same length (padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = datatools.preprocess(processed_words, tokenizer)\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every word now is a list of integers. We will shift the sequence one position, such that the target (to predict) is the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datatools.ShiftedDataset(padded_sequences)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset[0]\n",
    "print(f\"input: {x}\")\n",
    "print(f\"output: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use a Dataloader. This will batch the sequences and shuffle the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "x, y = next(iter(loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will use the vocabulary size to use as an output size for the model.\n",
    "The model now takes:\n",
    "- as input: a sequence of integers\n",
    "- as output: for every possible BPE token, the probability that it is the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocab size based on the tokenizer\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set up all the ingredients:\n",
    "- the model uses 64 dimensions to represent the language\n",
    "- we can calculate the loss (the difference between the actual next token and the predicted next token)\n",
    "- the optimizer will tell the model in which direction to adjust the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "from torch import nn, optim\n",
    "# Hyperparameters\n",
    "config = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"num_layers\": 2,\n",
    "    \"output_dim\": vocab_size,\n",
    "}\n",
    "\n",
    "model = models.SlangRNN(config)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=50, min_lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train for 800 epochs. This means we will present the full dataset of 453 words for 800 times to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from loguru import logger\n",
    "import torch\n",
    "\n",
    "# Set number of training epochs\n",
    "epochs = 800\n",
    "history = []\n",
    "last_lr = 0\n",
    "\n",
    "# Main training loop - iterate through epochs\n",
    "# an epoch is a complete pass through the dataset\n",
    "for epoch in range(epochs):\n",
    "   loss = 0\n",
    "\n",
    "   # Inner loop - process each batch of data\n",
    "   # for every batch of 32 samples, we update the model parameters\n",
    "   for x, y in loader:\n",
    "       # Clear gradients from previous batch\n",
    "       optimizer.zero_grad()\n",
    "\n",
    "       # Initialize hidden state for RNN/LSTM\n",
    "       hidden = model.init_hidden(x)\n",
    "\n",
    "       # Forward pass - get model predictions for next letters\n",
    "       output, hidden = model(x, hidden)\n",
    "\n",
    "       # Calculate loss by comparing predictions to targets\n",
    "       # Reshape output and target tensors to match expected dimensions\n",
    "       loss += loss_fn(output.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "   # Backward pass - compute gradients\n",
    "   loss.backward()\n",
    "   # Update model parameters using optimizer\n",
    "   # this is where the model learns by backpropagating\n",
    "   optimizer.step()\n",
    "\n",
    "   # Adjust learning rate based on loss\n",
    "   scheduler.step(loss)\n",
    "\n",
    "   # Store loss value for plotting/monitoring\n",
    "   history.append(loss.item())\n",
    "\n",
    "   # Get current learning rate from scheduler\n",
    "   curr_lr = scheduler.get_last_lr()\n",
    "\n",
    "   # Log training progress every 10 epochs\n",
    "   if (epoch+1) % 10 == 0:\n",
    "       logger.info(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "       # Log if learning rate has changed\n",
    "       if last_lr != curr_lr:\n",
    "           last_lr = curr_lr\n",
    "           logger.info(f\"Current learning rate: {curr_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if we have been learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylim(0, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save, and load, the trained model for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = Path(\"artefacts\")\n",
    "if not modeldir.exists():\n",
    "    modeldir.mkdir(parents=True)\n",
    "\n",
    "modelfile = modeldir / \"model.pth\"\n",
    "torch.save(model.state_dict(), modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.SlangRNN(config)\n",
    "model.load_state_dict(torch.load(modelfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now give a starting letter, eg 'a', and give the model a sequence of start_token and start_letter.\n",
    "The model will now start to predict next tokens, until it predicts the stop_token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generation parameters\n",
    "start_letter = 'a'\n",
    "max_length = 20\n",
    "temperature = 1.0\n",
    "\n",
    "# Get token indices for start token and first letter\n",
    "# we now have \"<s>a\" as the initial input sequence\n",
    "start_token_idx = tokenizer.encode(\"<s>\").ids[0]\n",
    "start_letter_idx = tokenizer.encode(start_letter).ids[0]\n",
    "\n",
    "# Create initial input sequence\n",
    "# we translate \"<s>a\" to tokens (numbers)\n",
    "input_seq = torch.tensor([[start_token_idx, start_letter_idx]], dtype=torch.long)\n",
    "generated_word = [start_letter_idx]\n",
    "print(f\"Initial input sequence: {input_seq}\")\n",
    "\n",
    "# Initialize model's hidden state\n",
    "hidden = model.init_hidden(input_seq)\n",
    "\n",
    "# Generate remaining characters one by one\n",
    "for _ in range(max_length - 1):\n",
    "   # Get model prediction without computing gradients\n",
    "   with torch.no_grad():\n",
    "       output, hidden = model(input_seq, hidden)\n",
    "\n",
    "   # Process model output and apply temperature scaling\n",
    "   output = output.squeeze(0)\n",
    "   output = output[-1, :].view(-1).div(temperature).exp()\n",
    "\n",
    "   # Sample next token based on model probabilities\n",
    "   next_token = torch.multinomial(output, 1).item()\n",
    "\n",
    "   # Stop if padding token is generated\n",
    "   if next_token == tokenizer.token_to_id(\"<pad>\"):\n",
    "       break\n",
    "\n",
    "   # Add token to generated sequence and update input\n",
    "   generated_word.append(next_token)\n",
    "   input_seq = torch.tensor([generated_word], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can decode into a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop this process to generate multiple words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.sample_n(processed_words, n=10, model=model, tokenizer=tokenizer, max_length=20, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save everything for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_file = modeldir / \"tokenizer.json\"\n",
    "tokenizer.save(str(tokenizer_file))\n",
    "torch.save(model.state_dict(), modelfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
